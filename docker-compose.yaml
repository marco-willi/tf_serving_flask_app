version: '3'

services:

  tf-serving-server:
    container_name: tf-serving-server
    image: ubuntu/tensorflow-serving-devel-cpu
    volumes:
      - ~/:/host
    command: bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=species_model --model_base_path=/host/data_hdd/ctc/ss/example/deploy_estimator &> estimator_log &
    networks:
      - tf_serving
    ports:
      - "9000:9000"

  tf-serving-client:
    container_name: tf-serving-client
    image: ubuntu/tensorflow-serving-client:latest
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ~/:/host
    networks:
      - tf_serving
    ports:
      - "5000:5000"
    environment:
      - TF_SERVER_NAME=tf-serving-server
      - TF_SERVER_PORT=9000
      - FLASK_SERVER_NAME=0.0.0.0
      - FLASK_SERVER_PORT=5000
      - FLASK_DEBUG=1
    depends_on:
      - tf-serving-server

networks:
  tf_serving:
    driver: bridge
